{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YongchaoLou/Yonghcao-Lou-Literature-Review/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKyiDqEDTSrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyjdrbRyTTnv",
        "colab_type": "text"
      },
      "source": [
        "Literature Review About Support Vector Machine For Pattern Recognition\n",
        "\n",
        "Yongchao Lou\n",
        "\n",
        "Introduction\n",
        " \n",
        "This literature review is reviewing a research paper about Support Vector Machine (SVM) for pattern recognition. The following paragraphs are going to discuss the method of Support Vector Machine, what is new about this method, other applications of Support Vector Machine, and how was the presentation of the whole research paper. \n",
        " \n",
        "There are several maths equations involved, and this literature review is not going to explain every equation in details. However, this literature review is going to explain briefly about Support Vector Machine (SVM).\n",
        " \n",
        "To explain SVM clearly, some essential word explanations are going to be included in the text.\n",
        " \n",
        "Content \n",
        "The paper mainly talked about how the Support Vector Machine (SVM) works. In this literature review, I am not going to explain the Support Vector Machine in detail, but talk about it in a concise and easy understanding way without mathematics involves. \n",
        " \n",
        "I prefer to call Support Vector Machine a Large Margin Classifier because that is what SVM do. The paper mentioned a picture which expresses the linearly separable case perfectly.  \n",
        "There are many ways to separate these two group of points with a straight line, however, from intuition, we felt that line A and line B are not the best way to separate the ends, line C seems to be a better way to separate them. It is because line C has the longest shortest distance from the points on both sides that closest to the other side. It sounds a little confusing, but it is clear when we draw a picture of it. \n",
        "\n",
        "The shortest distance from line C, “the best classifier,” to the points that are closest to line C called “Margin.” Line C is also called “SVM Decision Boundary.” All the above is the simplest case for SVM. \n",
        " \n",
        "Now we need to concern about a more difficult case: when there is an outlier appears, like the picture below:\n",
        "\n",
        "\n",
        "Decision boundary needs to be changed to separate the points perfectly. However, it is not a reasonable change. As a result, we need to allow SVM to have some “grey area,” which means we are not going to get the perfect result but still maintain a reasonable decision boundary. \n",
        " \n",
        "An even more complicated case for SVM is the non-linearly separable case. There are many cases that are not able to be “cut” into two groups with a straight line. In this case, SVM will use a technique called “Kernels” to solve this kind of questions. The kernel is a way to reflect low dimension to high dimension to find a hyperplane to separate the points. For P4, we can make it into a 3D model, and then we can find a hyperplane to separate these points. (csdn.net 2018, Ng, A. 2019).\n",
        " \n",
        " \n",
        "Innovation\n",
        " \n",
        "Before introducing the innovation part of the Support Vector Machine, we need to add some keywords.\n",
        " \n",
        "Empirical risk is the sample errors that trained classifiers made when re-classifying the training sets, which usually, most classifiers will have 100% correct after training (csdn.net 2017).\n",
        " \n",
        "Growth function means the most significant number for marking m training samples by hypothesis space H. For example, and if 3 points can be characterized as A and B, there are eight ways to mark them: AAA, AAB, ABA, BAA, ABB, BAB, BBB. The growth function is 8 in this case (csdn.net 2017).\n",
        " \n",
        "Dichotomy is every possible result for m training samples in data set D for hypothesis space H (csdn.net 2017).\n",
        " \n",
        "Shattering means hypothesis space H can make dichotomy for every samples for data set D, which means growth function is equal to 2^m, m is the number of training samples in data set D. However, in most cases, data sets cannot be shattered, which means growth function cannot reach 2^m (csdn.net 2017).\n",
        " \n",
        "Vapink-Chervonenkis Dimension of hypothesis space H is the most extensive data set that can be shattered by hypothesis space H. If hypothesis space H can shatter any data set no matter how many samples in it, then the VC Dimension is infinite. Basically, the higher the VC dimension is, the higher the complexity hypothesis space H is. VC dimension, in some way, represents the complexity (csdn.net 2017).\n",
        " \n",
        "Vapink-Chervonenkis confidence is the errors that trained classifiers made when they are classifying unknown samples (csdn.net 2017).\n",
        " \n",
        "Structural risk minimization is described as “a general model of capacity control and provides a trade-off between hypothesis space complexity (the VC dimension of approximating functions) and the quality of fitting the training data (empirical error).” (Vapnik and Chervonekis, 1974). \n",
        " \n",
        "The innovation part of Support Vector Machine is it is a classifier that focuses on structural risk minimization, which not only concern about the trained samples, but also the unknown data. Common classifiers only focused on Empirical Risk Minimisation by increasing VC dimension of hypothesis space, which means the structural risk does not change or even higher. High VC dimension might cause overfitting, which means they will not perform well when these common classifiers are handling unknown data. \n",
        " \n",
        " \n",
        " \n",
        "Technical Quality\n",
        " \n",
        "The technical quality of the paper is generally high. The results are clearly expressed through equations. However, it is not friendly enough for people who do not know the Support Vector Machine before since it is hard to follow if the readers do not have enough mathematics foundation. I will still rank the technical quality as high since the equations are the clearest way of performing results and logic progress. \n",
        " \n",
        "Support Vector Machine is good at generating, and it is a useful tool when the number of features is small, and the number of training examples is medium like10 - 10000, SVM with Gaussian Kernel will be a nice choice to make. Otherwise, logistic regression can be a nice choice. However, the choices depend on the situations people are facing. \n",
        " \n",
        " \n",
        " \n",
        "Application and X-Factor\n",
        " \n",
        "The application of Support Vector Machine is limited but still prevalent in many areas. When the number of training samples is much smaller than the number of features, as the training samples is less that we are not able to build complex non-linear models, or oppositely, when the number of features is small, and the number of training samples is large, Logistic Regression will perform better than Support Vector Machine. As the previous section mentioned, when the number of features is small, and the number of training samples is medium, then Support Vector Machine with Gaussian Kernel is suitable. \n",
        " \n",
        "There is still some areas of development for Support Vector Machine. The choose of kernels is still a problem for different situations, and there are no certain rules that can apply to make decisions between different kernels. Another limitation is Support Vector Machine is not suitable for large data set since the speed will be low and inefficient. \n",
        " \n",
        "I found interesting in work is because of the kernel. The kernel is an interesting way of reflecting low dimensions to high dimensions to solve the problems that are hard to solve on low dimensions. Support Vector Machine is a method with high potential. \n",
        " \n",
        "Presentation\n",
        " \n",
        "The presentation is friendly to readers who have some foundation of Machine Learning and Mathematics. The paper clearly explained every equation it used, and it is easy to follow with some foundation of mathematics. To readers who do not have enough knowledge, it is a hard task to understand Support Vector Machine with this paper, however, since the target audience is people with a foundation of Machine Learning, this paper did an excellent job. I will rate the presentation of this paper 4 out of 5. \n",
        "\n",
        "\n",
        "Citation\n",
        "\n",
        "Blog.csdn.net. 2015. 从VC维和结构风险最小原理深入理解SVM - 木东的博客 - CSDN博客. [online] Available at: https://blog.csdn.net/u010159842/article/details/46493815 [Accessed 28 Aug. 2019].\n",
        "\n",
        "Blog.csdn.net. 2017. 机器学习中的VC维数和分类 - Machine Learning with Peppa - CSDN博客. [online] Available at: https://blog.csdn.net/qq_39521554/article/details/78877567 [Accessed 28 Aug. 2019].\n",
        "\n",
        "Burges. 1998. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2, 121-167. \n",
        "\n",
        "Ng, A. 2019. Machine Learning | Coursera. [online] Coursera. Available at: https://www.coursera.org/learn/machine-learning?action=enroll [Accessed 28 Aug. 2019].\n",
        "\n",
        "Vapink, V. and Chervonekis, 1974. Structural risk minimization\n",
        "\n",
        "\n",
        "https://github.com/junjy007/UTS_ML2019_Main/blob/master/A1_Specification.pdf"
      ]
    }
  ]
}